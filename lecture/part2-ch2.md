# Chapter 2. 딥러닝이 잘 되는 이유를 이해하기 요약

이 문서는 딥러닝이 성공할 수 있었던 핵심 이유들을 기술적으로 설명하고 있습니다.
주요 내용은 1960년대 XOR 문제로 인해 좌절되었던 인공지능이 어떻게 다층 퍼셉트론과 오차 역전파 알고리즘을 통해 부활할 수 있었는지, 그리고 활성화 함수와 최적화 알고리즘의 발전 과정을 다루고 있습니다.
특히 딥러닝 성공의 3대 요인(알고리즘, 하드웨어, 데이터 혁신)을 명확히 제시하여 현재 딥러닝이 왜 이토록 성공적인지를 이해할 수 있도록 구성되어 있습니다.

## 학습 목표
- 다층 퍼셉트론의 개념과 등장 배경
- XOR 문제의 해결 방법
- 오차 역전파 알고리즘의 이해
- 딥러닝이 성공한 이유 분석

## 1. 다층 퍼셉트론의 등장

### XOR 문제 해결을 위한 두 가지 방법
1. **다층 퍼셉트론 (Multi-Layer Perceptron)**
2. **오차 역전파 (Backpropagation)**

### 다층 퍼셉트론의 개념
- 단일 퍼셉트론으로 해결할 수 없는 문제를 해결하기 위한 구조
- 여러 층의 뉴런을 쌓아서 복잡한 패턴 학습 가능

### 시각적 이해를 위한 예시
**문제 1**: 검은 점과 흰 점을 한 번에 나누어 보세요
- 단일 직선으로는 불가능
- 여러 직선의 조합으로 해결 가능

**문제 2**: 성냥개비 6개로 정삼각형 4개를 만들어 보세요
- 2차원적 사고로는 불가능
- 3차원적 사고로 해결 가능

## 2. XOR 문제의 해결

### 다층 퍼셉트론 구조
```
입력층 → 은닉층 → 출력층
```

### XOR 문제 해결 과정
1. **은닉층(Hidden Layer)** 추가
2. 은닉층에서 복잡한 패턴 학습
3. 출력층에서 최종 결과 도출

### 학습 과정
- 입력 데이터가 은닉층을 거쳐 출력층으로 전달
- 각 층에서 가중치와 편향을 통해 변환
- 최종 출력과 실제값 비교하여 오차 계산

## 3. 오차 역전파 - 딥러닝의 태동

### 오차 역전파의 개념
- **Forward Pass**: 입력에서 출력으로 데이터 전달
- **Backward Pass**: 출력에서 입력으로 오차 전달
- 각 층의 가중치를 역방향으로 조정

### 오차 역전파 공식
**핵심 공식**: `오차 × out × (1-out)`
- 시그모이드 함수의 미분을 이용한 가중치 업데이트
- 연쇄법칙(Chain Rule)을 통한 gradient 계산

### 학습 과정
1. **실제값과 예측값 비교**
2. **오차 계산**
3. **역방향으로 오차 전파**
4. **가중치 업데이트**
5. **반복 학습**

## 4. 핵심 개념 재정리

### 딥러닝의 3대 핵심 요소

#### 활성화 함수 (Activation Function)
- **정의**: 출력을 결정하는 함수
- **예시**: 일차 함수, 시그모이드 함수
- **발전**: 시그모이드 → ReLU → Leaky ReLU → ELU 등

#### 손실 함수 (Loss Function)
- **정의**: 예측값과 실제값 사이의 차이를 측정하는 함수
- **예시**: 이차 함수, 교차 엔트로피 함수

#### 최적화 알고리즘 (Optimizer)
- **정의**: 가중치를 조정하여 손실 함수를 최소화하는 알고리즘
- **발전**: 경사하강법 → SGD → Adam → AdamW 등

## 5. 활성화 함수의 발전

### 발전 과정
1. **시그모이드 함수**: 초기 활성화 함수
2. **ReLU**: Gradient Vanishing 문제 해결
3. **Leaky ReLU**: Dead ReLU 문제 해결
4. **ELU, Swish** 등: 더 나은 성능을 위한 개선

### 각 함수의 특징
- **시그모이드**: 0~1 사이 값, Gradient Vanishing 문제
- **ReLU**: 계산 효율성, 희소성 제공
- **개선된 함수들**: 각각의 단점을 보완

## 6. 최적화 알고리즘의 발전

### 발전 과정
1. **경사하강법 (Gradient Descent)**: 기본 최적화 방법
2. **SGD (Stochastic Gradient Descent)**: 확률적 경사하강법
3. **Adam**: 적응적 학습률 조정
4. **AdamW**: Weight Decay 추가

### 성능 개선 요소
- 학습률 적응적 조정
- 모멘텀 개념 도입
- 가중치 감소 기법 적용

## 7. 딥러닝의 구조와 기법

### 주요 구성 요소
- **Dropout**: 과적합 방지
- **Batch Normalization**: 학습 안정화
- **Regularization**: 일반화 성능 향상
- **다양한 층 구조**: CNN, RNN, Transformer 등

### 강화학습과의 연결
- **Environment**: 환경
- **State**: 상태
- **Action**: 행동
- **Reward**: 보상

## 8. 딥러닝이 잘 되는 이유

### 3가지 핵심 혁신

#### 1. 알고리즘의 혁신
- **다층 퍼셉트론**: 복잡한 패턴 학습 가능
- **오차 역전파**: 효율적인 학습 알고리즘
- **다양한 아키텍처**: CNN, RNN, Transformer 등

#### 2. 장비의 혁신
- **GPU**: 병렬 처리로 학습 속도 대폭 향상
- **TPU**: 딥러닝 전용 프로세서
- **클라우드 컴퓨팅**: 접근성 향상

#### 3. 데이터의 혁신
- **빅데이터**: 대용량 학습 데이터 확보
- **인터넷**: 다양한 데이터 소스
- **데이터 처리 기법**: 전처리, 증강 기법 발달

## 결론

딥러닝의 성공은 **알고리즘**, **하드웨어**, **데이터**의 삼박자가 맞아떨어진 결과입니다. 특히 1960년대 퍼셉트론의 한계를 극복하기 위해 개발된 다층 퍼셉트론과 오차 역전파 알고리즘이 현재 딥러닝의 기초가 되었으며, GPU와 빅데이터의 등장으로 실용적인 수준까지 발전할 수 있었습니다.

---

*실습 자료: https://github.com/taehojo/fastcampus_ai*
*참고 도서: 모두의 딥러닝 (조태호, 길벗, 2022)*