{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v0_caBv3uJCn"
   },
   "source": [
    "# 12 트랜스포머의 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nevncdxVuJCv",
    "outputId": "2a6d1aaa-e54e-4b96-c4ad-c0302717048d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 2s 10ms/step - loss: 0.6190 - accuracy: 0.6442 - val_loss: 0.0344 - val_accuracy: 0.9900\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0484 - accuracy: 0.9894 - val_loss: 0.0207 - val_accuracy: 0.9975\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0333 - accuracy: 0.9956 - val_loss: 0.0242 - val_accuracy: 0.9975\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0381 - accuracy: 0.9950 - val_loss: 0.0207 - val_accuracy: 0.9975\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0346 - accuracy: 0.9931 - val_loss: 0.0202 - val_accuracy: 0.9925\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0254 - accuracy: 0.9937 - val_loss: 0.0103 - val_accuracy: 0.9975\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0174 - accuracy: 0.9950 - val_loss: 0.0060 - val_accuracy: 0.9975\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0167 - accuracy: 0.9962 - val_loss: 0.0248 - val_accuracy: 0.9925\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0129 - accuracy: 0.9975 - val_loss: 8.7601e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0247 - accuracy: 0.9950 - val_loss: 0.0091 - val_accuracy: 0.9975\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Text: I absolutely love this!\n",
      "Prediction: Positive\n",
      "Text: I can't stand this product\n",
      "Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CSV 파일 로드\n",
    "dataframe = pd.read_csv('../data/sentiment_data.csv')\n",
    "\n",
    "# 데이터와 라벨 추출\n",
    "sentences = dataframe['sentence'].tolist()\n",
    "labels = dataframe['label'].tolist()\n",
    "\n",
    "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
    "embedding_dim = 128\n",
    "max_len = 10\n",
    "\n",
    "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 포지셔널 인코딩 함수\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    pos_enc = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))\n",
    "    return pos_enc\n",
    "\n",
    "# 포지셔널 인코딩 생성\n",
    "positional_encoding = get_positional_encoding(max_len, embedding_dim)\n",
    "\n",
    "# 사용자 정의 레이어: MultiHeadAttention을 포함한 레이어 정의\n",
    "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        # 초기화 메서드에서 MultiHeadSelfAttentionLayer 클래스의 인스턴스를 초기화합니다.\n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()\n",
    "        # 멀티헤드 어텐션 레이어를 생성합니다.\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        # 레이어 정규화를 생성합니다.\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        # 입력 x에 대해 멀티헤드 어텐션을 적용합니다.\n",
    "        # query, value, key 모두 동일한 x를 사용합니다.\n",
    "        attn_output = self.mha(query=x, value=x, key=x)\n",
    "        # 잔차 연결을 적용하여 입력 x와 어텐션 출력 attn_output을 더한 후, 정규화를 적용합니다.\n",
    "        attn_output = self.norm(attn_output + x)\n",
    "        # 어텐션과 정규화된 출력을 반환합니다.\n",
    "        return attn_output\n",
    "\n",
    "# 모델 설정\n",
    "inputs = Input(shape=(max_len,))\n",
    "\n",
    "# 1. 임베딩 레이어: 텍스트 데이터를 임베딩 벡터로 변환합니다.\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_len)\n",
    "embedded_sequences = embedding_layer(inputs)\n",
    "\n",
    "# 2. 포지셔널 인코딩 추가\n",
    "embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding\n",
    "\n",
    "# 3. 멀티헤드 어텐션 레이어 추가\n",
    "attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)\n",
    "attention_output = attention_layer(embedded_sequences_with_positional_encoding)\n",
    "\n",
    "# 4. 잔차 연결\n",
    "attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])\n",
    "\n",
    "# 5. GlobalAveragePooling1D 레이어 추가\n",
    "pooled_output = GlobalAveragePooling1D()(attention_output_with_residual)\n",
    "\n",
    "# 6. 피드 포워드 네트워크\n",
    "dense_layer = Dense(128, activation='relu')(pooled_output)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "# 모델 컴파일\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, np.array(y_train), epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)))\n",
    "\n",
    "# 샘플 데이터 예측\n",
    "sample_texts = [\"I absolutely love this!\", \"I can't stand this product\"]\n",
    "sample_sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "sample_data = tf.keras.preprocessing.sequence.pad_sequences(sample_sequences, maxlen=max_len, padding='post')\n",
    "predictions = model.predict(sample_data)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "12-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
