{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v0_caBv3uJCn"
   },
   "source": [
    "# 6교시 3.트랜스포머의 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nevncdxVuJCv",
    "outputId": "2a6d1aaa-e54e-4b96-c4ad-c0302717048d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'data' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 3s 19ms/step - loss: 0.7979 - accuracy: 0.5394 - val_loss: 0.7066 - val_accuracy: 0.4975\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.4472 - accuracy: 0.7644 - val_loss: 0.1477 - val_accuracy: 0.9800\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0759 - accuracy: 0.9875 - val_loss: 0.0083 - val_accuracy: 0.9975\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0454 - accuracy: 0.9937 - val_loss: 0.0134 - val_accuracy: 0.9950\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0300 - accuracy: 0.9944 - val_loss: 0.0108 - val_accuracy: 0.9975\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0225 - accuracy: 0.9956 - val_loss: 0.0143 - val_accuracy: 0.9950\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0092 - accuracy: 0.9987 - val_loss: 0.0158 - val_accuracy: 0.9975\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 0.9975\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0095 - accuracy: 0.9981 - val_loss: 1.1306e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 3.1072e-04 - accuracy: 1.0000 - val_loss: 7.6183e-04 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "Text: I absolutely love this!\n",
      "Prediction: Positive\n",
      "Text: I can't stand this product\n",
      "Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 깃허브에 준비된 데이터를 가져옵니다.\n",
    "!git clone https://github.com/taehojo/data.git\n",
    "\n",
    "# CSV 파일 로드\n",
    "dataframe = pd.read_csv('./data/sentiment_data.csv')\n",
    "\n",
    "# 데이터와 라벨 추출\n",
    "sentences = dataframe['sentence'].tolist()\n",
    "labels = dataframe['label'].tolist()\n",
    "\n",
    "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
    "embedding_dim = 128\n",
    "max_len = 10\n",
    "\n",
    "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 포지셔널 인코딩 함수\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    pos_enc = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))\n",
    "    return pos_enc\n",
    "\n",
    "# 포지셔널 인코딩 생성\n",
    "positional_encoding = get_positional_encoding(max_len, embedding_dim)\n",
    "\n",
    "# 사용자 정의 레이어: MultiHeadAttention을 포함한 레이어 정의\n",
    "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, masked=False):\n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.norm = LayerNormalization()\n",
    "        self.masked = masked\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.masked:\n",
    "            # 마스크드 어텐션을 적용할 경우\n",
    "            batch_size = tf.shape(x)[0]  # 입력 x의 배치 크기\n",
    "            seq_len = tf.shape(x)[1]     # 입력 x의 시퀀스 길이\n",
    "    \n",
    "            # 현재 시점 이후의 정보를 가릴 수 있도록 마스크 행렬 생성\n",
    "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # 대각선 이하를 1로 채우기\n",
    "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))  # 행렬을 [1, 1, seq_len, seq_len] 형태로 변형\n",
    "            mask = tf.tile(mask, [batch_size, 1, 1, 1])  # 배치 크기만큼 마스크를 반복하여 확장\n",
    "            \n",
    "            # 마스크 행렬을 -무한대로 변경\n",
    "            mask = mask * -1e9\n",
    "    \n",
    "            # 마스크를 사용한 멀티헤드 어텐션\n",
    "            attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "        else:\n",
    "            # 마스크 없이 멀티헤드 어텐션을 적용할 경우\n",
    "            attn_output = self.mha(query=x, value=x, key=x)\n",
    "        \n",
    "        # 잔차 연결(residual connection) 후 LayerNormalization 적용\n",
    "        attn_output = self.norm(attn_output + x)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "# 모델 설정\n",
    "inputs = Input(shape=(max_len,))\n",
    "\n",
    "# 1. 임베딩 레이어: 텍스트 데이터를 임베딩 벡터로 변환합니다.\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_len)\n",
    "embedded_sequences = embedding_layer(inputs)\n",
    "\n",
    "# 2. 포지셔널 인코딩 추가\n",
    "embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding\n",
    "\n",
    "# 3. 멀티헤드 어텐션 레이어 추가\n",
    "attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)\n",
    "attention_output = attention_layer(embedded_sequences_with_positional_encoding)\n",
    "\n",
    "# 4. 잔차 연결\n",
    "attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])\n",
    "\n",
    "# 5. 마스크드 멀티헤드 어텐션 레이어 추가 (디코더)\n",
    "masked_attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim, masked=True)\n",
    "masked_attention_output = masked_attention_layer(attention_output_with_residual)\n",
    "\n",
    "# 6. 잔차 연결\n",
    "masked_attention_output_with_residual = Add()([attention_output_with_residual, masked_attention_output])\n",
    "\n",
    "# 7. GlobalAveragePooling1D 레이어 추가\n",
    "pooled_output = GlobalAveragePooling1D()(masked_attention_output_with_residual)\n",
    "\n",
    "# 8. 피드 포워드 네트워크\n",
    "dense_layer = Dense(128, activation='relu')(pooled_output)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "# 모델 컴파일\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, np.array(y_train), epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)))\n",
    "\n",
    "# 샘플 데이터 예측\n",
    "sample_texts = [\"I absolutely love this!\", \"I can't stand this product\"]\n",
    "sample_sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "sample_data = tf.keras.preprocessing.sequence.pad_sequences(sample_sequences, maxlen=max_len, padding='post')\n",
    "predictions = model.predict(sample_data)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 특성과 모델 복잡성에 따라 마스크드 어텐션의 사용 여부를 결정해 최적의 모델을 선택하는 것이 중요합니다. 특히 마스크드 어텐션은 문장 순서가 중요한 경우에 유리합니다. 주어진 샘플의 경우, 아래와 같이 샘플 수를 줄이면 마스크드 어텐션 유무의 차이가 더욱 두드러지게 나타납니다. 이러한 요소가 주어진 데이터의 분석 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "<img src=\"../data/img/validation_accuracy_comparison_smooth.png\" alt=\"Validation Accuracy Comparison\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "12-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
