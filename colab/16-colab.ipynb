{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v0_caBv3uJCn"
   },
   "source": [
    "# 보충 마스크드 어텐션 전후 코드 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 100% - Validation Accuracy: 0.9975000023841858\n",
      "Sample Size: 20% - Validation Accuracy: 0.9975000023841858\n",
      "Sample Size: 10% - Validation Accuracy: 0.9775000214576721\n",
      "Sample Size: 5% - Validation Accuracy: 0.824999988079071\n",
      "Sample Size: 1% - Validation Accuracy: 0.5174999833106995\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CSV 파일 로드\n",
    "dataframe = pd.read_csv('sentiment_data.csv')\n",
    "\n",
    "# 데이터와 라벨 추출\n",
    "sentences = dataframe['sentence'].tolist()\n",
    "labels = dataframe['label'].tolist()\n",
    "\n",
    "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
    "embedding_dim = 128\n",
    "max_len = 10\n",
    "\n",
    "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 포지셔널 인코딩 함수\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    pos_enc = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))\n",
    "    return pos_enc\n",
    "\n",
    "# 포지셔널 인코딩 생성\n",
    "positional_encoding = get_positional_encoding(max_len, embedding_dim)\n",
    "\n",
    "# 사용자 정의 레이어: MultiHeadAttention을 포함한 레이어 정의\n",
    "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, value=x, key=x)\n",
    "        attn_output = self.norm(attn_output + x)\n",
    "        return attn_output\n",
    "\n",
    "# 모델 생성 함수\n",
    "def create_model():\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_len)\n",
    "    embedded_sequences = embedding_layer(inputs)\n",
    "    embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding\n",
    "    attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)\n",
    "    attention_output = attention_layer(embedded_sequences_with_positional_encoding)\n",
    "    attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output_with_residual)\n",
    "    dense_layer = Dense(128, activation='relu')(pooled_output)\n",
    "    dropout_layer = Dropout(0.5)(dense_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 샘플 양을 100%, 20%, 10%, 5%, 1%씩 줄였을 때의 정확도 추적\n",
    "accuracies = []\n",
    "sample_sizes = [1.0, 0.2, 0.1, 0.05, 0.01]\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    # 데이터셋의 일부를 샘플링\n",
    "    sample_indices = np.random.choice(len(X_train), int(len(X_train) * sample_size), replace=False)\n",
    "    X_train_sample = X_train[sample_indices]\n",
    "    y_train_sample = np.array(y_train)[sample_indices]\n",
    "\n",
    "    # 모델 생성 및 컴파일\n",
    "    model_sample = create_model()\n",
    "    \n",
    "    # 모델 학습\n",
    "    history_sample = model_sample.fit(X_train_sample, y_train_sample, epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)), verbose=0)\n",
    "\n",
    "    # 샘플링된 데이터셋에서의 정확도 기록\n",
    "    accuracies.append(max(history_sample.history['val_accuracy']))\n",
    "\n",
    "# 정확도 출력\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    print(f\"Sample Size: {int(sample_size * 100)}% - Validation Accuracy: {accuracies[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 100% - Validation Accuracy: 1.0\n",
      "Sample Size: 20% - Validation Accuracy: 0.9925000071525574\n",
      "Sample Size: 10% - Validation Accuracy: 0.6850000023841858\n",
      "Sample Size: 5% - Validation Accuracy: 0.5174999833106995\n",
      "Sample Size: 1% - Validation Accuracy: 0.5174999833106995\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CSV 파일 로드\n",
    "dataframe = pd.read_csv('sentiment_data.csv')\n",
    "\n",
    "# 데이터와 라벨 추출\n",
    "sentences = dataframe['sentence'].tolist()\n",
    "labels = dataframe['label'].tolist()\n",
    "\n",
    "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
    "embedding_dim = 128\n",
    "max_len = 10\n",
    "\n",
    "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 포지셔널 인코딩 함수\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    pos_enc = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))\n",
    "    return pos_enc\n",
    "\n",
    "# 포지셔널 인코딩 생성\n",
    "positional_encoding = get_positional_encoding(max_len, embedding_dim)\n",
    "\n",
    "# 사용자 정의 레이어: MultiHeadAttention을 포함한 레이어 정의\n",
    "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, masked=False):\n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.norm = LayerNormalization()\n",
    "        self.masked = masked\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.masked:\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            seq_len = tf.shape(x)[1]\n",
    "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))\n",
    "            mask = tf.tile(mask, [batch_size, 1, 1, 1])\n",
    "            attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "        else:\n",
    "            attn_output = self.mha(query=x, value=x, key=x)\n",
    "        attn_output = self.norm(attn_output + x)\n",
    "        return attn_output\n",
    "\n",
    "# 모델 생성 함수\n",
    "def create_model():\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_len)\n",
    "    embedded_sequences = embedding_layer(inputs)\n",
    "    embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding\n",
    "    attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)\n",
    "    attention_output = attention_layer(embedded_sequences_with_positional_encoding)\n",
    "    attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])\n",
    "    masked_attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim, masked=True)\n",
    "    masked_attention_output = masked_attention_layer(attention_output_with_residual)\n",
    "    masked_attention_output_with_residual = Add()([attention_output_with_residual, masked_attention_output])\n",
    "    pooled_output = GlobalAveragePooling1D()(masked_attention_output_with_residual)\n",
    "    dense_layer = Dense(128, activation='relu')(pooled_output)\n",
    "    dropout_layer = Dropout(0.5)(dense_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 샘플 양을 100%, 20%, 10%, 5%, 1%씩 줄였을 때의 정확도 추적\n",
    "accuracies = []\n",
    "sample_sizes = [1.0, 0.2, 0.1, 0.05, 0.01]\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    # 데이터셋의 일부를 샘플링\n",
    "    sample_indices = np.random.choice(len(X_train), int(len(X_train) * sample_size), replace=False)\n",
    "    X_train_sample = X_train[sample_indices]\n",
    "    y_train_sample = np.array(y_train)[sample_indices]\n",
    "\n",
    "    # 모델 생성 및 컴파일\n",
    "    model_sample = create_model()\n",
    "    \n",
    "    # 모델 학습\n",
    "    history_sample = model_sample.fit(X_train_sample, y_train_sample, epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)), verbose=0)\n",
    "\n",
    "    # 샘플링된 데이터셋에서의 정확도 기록\n",
    "    accuracies.append(max(history_sample.history['val_accuracy']))\n",
    "\n",
    "# 정확도 출력\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    print(f\"Sample Size: {int(sample_size * 100)}% - Validation Accuracy: {accuracies[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/img/validation_accuracy_comparison_smooth.png\" alt=\"Validation Accuracy Comparison\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "12-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
